---
title: "Task 1 - ST443 Project"
author: "Finbar Rhodes"
date: "`r Sys.Date()`"
output: html_document
---

```{r libraries, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(443)
library(ggplot2)
library(tidyverse)
library(MASS)
library(class)
library(pROC)
library(caret) 
library(mboost)   
library(gbm)
library(xgboost)
library(ranger)
library(e1071)
library(lattice)
library(PRROC)
```

```{r reading in data, echo=TRUE}
task1data <- read.csv("data1.csv.gz")
task1data$label <- if_else(task1data$label == "TREG", 1, 0)
```


```{r eval_metrics function, echo=TRUE}

eval_metrics <- function(str, conf){
  
  ### function takes as input a string (str) that describes the method, and a 
  ### a confusion matrix to evaluate Accuracy, Balanced, Precision, Recall, 
  ### and F1 score from. 
  ###
  ### Returns a row of a dataframe with the method and the key metrics.
  
  # setup
  TN <- conf[[1]]
  FN <- conf[[2]]
  FP <- conf[[3]]
  TP <- conf[[4]]
  
  # metrics
  accuracy <- 1 - ((FN + FP) / sum(conf))
  BA <- .5 * (TP / (TP + FN)) + .5 * (TN / (TN + FP))
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall + 1e-6)
  
  # output
  return(data.frame("Method" = str,
                    "Accuracy" = accuracy |> round(digits = 3),
                    "Balanced Accuracy" = BA |> round(digits = 3),
                    "F1" = F1 |> round(digits = 3)))
}

```


## T1.1

```{r missing data, echo=TRUE}

nas <- task1data |> is.na() |> colSums() |> table()
if (nas[1] == ncol(task1data)){
  print("We have that there are no missing data in any columns which will be helpful in our analysis")
}
```

```{r overall sparsity, echo=FALSE}
total_zeros <- sum(rowSums(task1data == 0))
total_entries <- nrow(task1data) * ncol(task1data)
(total_zeros / total_entries) |> round(digits = 3)
```

Our given data is reasonably sparse. In fact, below we have shown that approximately 66.2% of this dataset are zero entries. We can explore sparsity across the covariates as well.

```{r sparsity across features, echo=TRUE}
covariate_sparsities <- data.frame(Gene = colnames(task1data)[-1], 
                                   Sparsity = rep(0, ncol(task1data)-1)) 

for (i in 2:ncol(task1data)){
  count <- length(which(task1data[,i] == 0))
  covariate_sparsities$Sparsity[i-1] <- count / nrow(task1data)
}

summary(covariate_sparsities)
ggplot(covariate_sparsities, aes(Sparsity)) + 
  geom_histogram(color = "black", fill = "white" ,bins = 40) + 
  ggtitle("Distribution of Sparsity Rates Across Covariates") + 
  geom_vline(xintercept = mean(covariate_sparsities$Sparsity), color = "red", linetype = "dashed") +
  theme_bw()

barplot(colMeans(task1data[2:ncol(task1data)]))

```


```{r heatmap}

task1matrix <- sapply(task1data, as.numeric) |> as.matrix()

pal <- colorRampPalette(c("red", "yellow"), space = "rgb") 
levelplot(task1matrix, 
          main="Task 1 Data Heatmap",
          xlab=" ", 
          ylab=" ", 
          col.regions=pal(40), 
          cuts=3, 
          at=seq(0,1,0.5))


```

## T1.2

Below we will shuffle the rows in our dataset, then split them into *training*, *validation*, and *test* sets. In the base and PCA model fits, we use *train* and *test*, and for the tuning portion (T1.3), we use the *validation* set.

```{r data setup, echo=TRUE}
task1data <- task1data[sample(1:nrow(task1data)), ]

training_index <- floor(nrow(task1data) * 0.7)  # 70% for training
validation_index <- floor(nrow(task1data) * 0.85)  # Next 15% for validation, leaving 15% for test set

# Split the data into training, validation, and testing sets
task1_train <- task1data[1:training_index, ]  # 70% of the data
task1_validation <- task1data[(training_index + 1):validation_index, ]  # 15% of the data
task1_test <- task1data[(validation_index + 1):nrow(task1data), ]  # Remaining 15% of the data

```

### Base Models

#### LDA

```{r lda fit}
lda_fit <- lda(label ~ ., data = task1_train)
```

```{r lda predict}
lda_pred_full <-  predict(lda_fit, task1_test)
lda_pred <- predict(lda_fit, task1_test)$class

lda_conf_matrix <- table(lda_pred, task1_test$label)
eval_metrics("lda", lda_conf_matrix)
```

```{r lda eval, echo=TRUE}
lda_conf_matrix
eval_metrics(str = "LDA", conf = lda_conf_matrix)

```

#### Logistic Regression

```{r logistic regression, echo=TRUE}
logistic_fit <- glm(label ~ ., data = task1_train, family = binomial)
logistic_probs <-  predict(logistic_fit, newdata = task1_test, type = "response")
```

```{r logit eval}
logistic_pred <-  rep(0, nrow(task1_test))
logistic_pred[logistic_probs > .5] <-  1

logistic_conf_matrix <- table(logistic_pred, task1_test$label)
eval_metrics(str = "Logisitc Regression", conf = logistic_conf_matrix)
```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods.

```{r qda, echo=TRUE}
#dummy_data <- task1data[,1:1624]
#qda_fit <- MASS::qda(label ~ ., data = dummy_data, subset = train)

```

#### KNN

```{r knn, echo=TRUE}
knn <- knn3(label ~., data = task1_train, k=1)
knn_preds <- predict(knn, newdata = task1_test, type = "prob")
knn_conf_matrix <- table(knn_preds[,2], task1_test$label)
eval_metrics("KNN", knn_conf_matrix)
```

#### GBDT

```{r GBDT}

gbdt <- gbm(label ~ ., data = task1_train, 
                   distribution = "bernoulli", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001)

gbdt_probs <- predict(gbdt, newdata = task1_test, n.trees = 1000, type = "response")

gbdt_preds <-  rep(0, nrow(task1_test))
gbdt_preds <- ifelse(gbdt_probs > .5, 1, 0)

gbdt_conf_matrix <- table(gbdt_preds, task1_test$label)

```

#### Random Forest

```{r echo=TRUE}

rf <- ranger(label~., 
             data = task1_train, 
             mtry = ncol(task1_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

rf_preds <- predict(rf, data=task1_test)$predictions

rf_conf_matrix <- table(rf_preds, task1_test$label)

```

#### SVM

```{r}
svmfit = svm(label ~ ., data = task1_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = 10)

svm_predictions <- predict(svmfit, newdata = task1_test[2:ncol(task1_test)], probabilities = TRUE)
svm_conf_matrix <- table(svm_predictions, task1_test$label)
```



## T1.2.PCA

```{r pca fit & data}
pca <- prcomp(~., data = task1data[2:ncol(task1data)])
top10_weights <- pca$rotation[,1:10]
reduced_data <- pca$x[,1:10] |> as.data.frame()
reduced_data$label <- task1data$label
```

```{r pca plot, echo=TRUE}
table_pca <- rbind(pca$rotation[,1:20], summary(pca)$importance[,1:20])


# var_props <- data.frame(PC = 1:10, 'PVE' = table_pca['Proportion of Variance',])
"
ggplot(var_props, aes(x=PC, y=PVE)) + 
  geom_line() + 
  geom_point() + 
  xlab('Principal Components') + 
  ylab('Proportion of Variance Remaining') + 
  xlim(0, .5)
  "



par(mfrow=c(1,1))
plot(table_pca['Proportion of Variance',], 
     type = 'l', 
     lwd = 5, 
     col = 'blue', 
     xlim = c(1,20), 
     ylim = c(0,.05),
     main = 'Proportion of Variance Explained by Principal Components', 
     xlab = 'Principal Components', 
     ylab = 'Proportion of Variance Unexplained', 
     axes = TRUE)

```

Same data set splitting as earlier; PCA maintains row order by matrix multiplication.

```{r data setup, echo=TRUE}
reduced_train <- reduced_data[1:training_index,] 
reduced_validation <- reduced_data[(training_index + 1):validation_index,]
reduced_test <- reduced_data[(validation_index + 1):nrow(task1data),] 
```

### Models with PCA

#### LDA

```{r pca lda fit}
pca_lda_fit <- lda(label ~ ., data = reduced_train)
```

```{r pca lda predict}
pca_lda_pred_full <-  predict(pca_lda_fit, reduced_test)
pca_lda_pred <- predict(pca_lda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_lda_conf_matrix <- table(pca_lda_pred, pca_test_labels)
```

```{r pca lda eval, echo=TRUE}
pca_lda_conf_matrix
eval_metrics(str = "LDA", conf = pca_lda_conf_matrix)

```

#### Logistic Regression

```{r pca logistic regression, echo=TRUE}
pca_logistic_fit <- glm(label ~ ., data = reduced_train, family = binomial)
pca_logistic_probs <-  predict(pca_logistic_fit, newdata = reduced_test, type = "response")
```

```{r pca logit eval}
pca_logistic_pred <-  rep(0, nrow(reduced_test))
pca_logistic_pred[pca_logistic_probs > .5] <-  1

# tail(cbind(task1_train$label,logistic_pred))
pca_logistic_conf_matrix <- table(pca_logistic_pred, reduced_test$label)
eval_metrics(str = "Logisitc Regression", conf = pca_logistic_conf_matrix)
```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods.

```{r qda, echo=TRUE}
pca_qda_fit <- MASS::qda(label ~ ., data = reduced_train)

pca_qda_pred_full <-  predict(pca_qda_fit, reduced_test)
pca_qda_pred <- predict(pca_qda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_qda_conf_matrix <- table(pca_qda_pred, pca_test_labels)

```

#### KNN

```{r pca knn, echo=TRUE}

pca_knn <- knn3(label ~., data = reduced_train, k = 1)
pca_knn_preds <- predict(pca_knn, newdata = reduced_test, type = "prob")
pca_knn_conf_matrix <- table(pca_knn_preds[,2], task1_test$label)
```

#### GBDT

```{r pca GBDT}

pca_gbdt <- gbm(label ~ ., data = reduced_train, 
                   distribution = "bernoulli", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001, 
                   cv.folds = 5)

pca_gbdt_probs <- predict(pca_gbdt, newdata = reduced_test, n.trees = 1000, type = "response")

pca_gbdt_preds <-  rep(0, nrow(task1_test))
pca_gbdt_preds <- ifelse(pca_gbdt_probs > .5, 1, 0)

pca_gbdt_conf_matrix <- table(pca_gbdt_preds, reduced_test$label)
```

#### Random Forest

```{r pca rf, echo=TRUE}

pca_rf <- ranger(label~., 
             data = reduced_train, 
             mtry = ncol(reduced_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

pca_rf_preds <- predict(pca_rf, data=reduced_test)$predictions

pca_rf_conf_matrix <- table(pca_rf_preds, reduced_test$label)

eval_metrics("RF", pca_rf_conf_matrix)

```

#### SVM

```{r pca svm}
pca_svmfit = svm(label ~ ., data = reduced_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = 10)

pca_svm_predictions <- predict(pca_svmfit, newdata = reduced_test, probabilities = TRUE)
pca_svm_conf_matrix <- table(pca_svm_predictions, reduced_test$label)
eval_metrics("PCA SVM", pca_svm_conf_matrix)

```


## T1.3

Classifiers to tweak:
- KNN via Tuning *k*
- GBDT via Tuning Depth, Shrinkage, and Decision Threshold
- SVM via Tuning Cost (Margin) Parameter

In this section, we bring in the validation set (15% of the data) to tune hyperparameters, settle on an optimal setup, and then reevaluate on the test set. 

#### KNN

In the *k*-Nearest Neighbor classifier, here we are tweaking the main parameter *k*, the number of neighbors taken into account in fitting the model.

```{r knn k-tuning, echo=TRUE}
knn_validation_preds <- predict(knn, newdata = task1_validation, type = "prob")
knn_validation_conf_matrix <- table(knn_validation_preds[,2], task1_test$label)
knn_metrics <- eval_metrics(str = "1 NN", conf = knn_validation_conf_matrix)

pca_knn_validation_preds <- predict(pca_knn, newdata = reduced_validation[1:ncol(reduced_validation)-1], type = "prob")
pca_knn_validation_conf_matrix <- table(pca_knn_validation_preds[,2], reduced_test$label)
pca_knn_metrics <- eval_metrics(str = "1 NN", conf = pca_knn_validation_conf_matrix)

for (k in 2:20){
  model <- knn3Train(task1_train[2:ncol(task1_train)], 
                     task1_validation[2:ncol(task1_validation)], 
                     task1_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)
  
  pca_model <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                     reduced_validation[2:ncol(reduced_validation)], 
                     reduced_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)  

  conf_matrix <- table(model, task1_validation$label)
  pca_conf_matrix <- table(pca_model, reduced_validation$label)
  
  
  knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), conf_matrix)
  pca_knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), pca_conf_matrix)
  
  print(k)
}

ggplot(knn_metrics, aes(x=(1:nrow(knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(knn_metrics$F1), linetype = "dashed", color = "red") + 
  xlim(1,nrow(knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

ggplot(pca_knn_metrics, aes(x=(1:nrow(pca_knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(pca_knn_metrics$F1), linetype = "dashed", color = "blue") + 
  xlim(1,nrow(pca_knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score for Dimension-Reduced Models") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

```

```{r final knn, echo=TRUE}
# KNN Model Predictions

tuned_1nn <- knn3Train(task1_train[2:ncol(task1_train)], 
                       task1_test[2:ncol(task1_test)], 
                       task1_train$label, 
                       k=5, 
                       prob = TRUE, 
                       use.all=TRUE)

tuned_1nn_conf_matrix <- table(tuned_1nn, task1_test$label)

pca_tuned_knn <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                       reduced_test[2:ncol(reduced_test)], 
                       reduced_train$label, 
                       k=9, 
                       prob = TRUE, 
                       use.all=TRUE)

pca_tuned_knn_conf_matrix <- table(pca_tuned_knn, reduced_test$label)

```

#### Random Forest COULD CHANGE TO SVM

```{r rf mtry tuning, echo=TRUE}

rf_metrics <- eval_metrics("Base Model", rf_conf_matrix)

parameter_pool_sizes <- (ncol(task1_train) |> sqrt() |> round(digits = 0)-25) : (ncol(task1_train) |> sqrt() |> round(digits = 0)+25)

for (i in 110:125){
  model <- ranger(label~., 
             data = task1_train, 
             mtry = i,
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE)

  preds <- predict(model, data=task1_test)$predictions

  conf_matrix <- table(preds, task1_test$label)
  
  rf_metrics <- rbind(rf_metrics, eval_metrics(paste("mtry =", as.character(i)), conf_matrix)) 
  
  print(i)
  
}


rf_metrics$index <- c(64, parameter_pool_sizes, 90:110, 110:125)

ggplot(rf_metrics, aes(x=index, F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(rf_metrics$F1) + 37, linetype = "dashed", color = "blue") + 
  #xlim(1,nrow(rf_metrics)) + 
  xlab("Feature Pool Size") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score for Dimension-Reduced Models") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 


```
```{r INEFFECTIVE rf threshold selection}

# Predict on the Validation Set

# Random Forest Model Predictions
rf_validation_preds <- predict(rf, data = task1_validation[2:ncol(task1_validation)], type = "response")$predictions

# Calculate PR Curves Using Validation Labels (y_val)

# PR Curve for Lasso    Precision-Recall
rf_validation_pr_curve <- pr.curve(scores.class0 = rf_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)

cat("PR AUC (Random Forest, Validation Set):", rf_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
rf_curve_df <- data.frame(Model = "Random Forest", 
                          Recall = rf_validation_pr_curve$curve[, 1], 
                          Precision = rf_validation_pr_curve$curve[, 2], 
                          Threshold = rf_validation_pr_curve$curve[, 3])

```

#### GBDT


One classifier to improve upon is Gradient Boosted Decision Trees. First, we will tune the shrinkage and interaction (tree) depth hyperparameters, and then we will use the optimal setup we found to tune the right decision threshold. 

```{r, GBTD hyperparameter tuning, echo=TRUE}
gbdt_metrics <- eval_metrics("Base Model", gbdt_conf_matrix)
# Define grid of lambda (shrinkage) values to evaluate
lambdas <- c(.001, .01, .025, .05)
depths <- c(1,2,3,4,5,6)
# Loop over each lambda value
for (lambda in lambdas) {
  for (depth in depths){
    # Train the gbm model with the current lambda (shrinkage) value
    gbm_model <- gbm(label ~ ., data = task1_train, 
                     distribution = "bernoulli", 
                     n.trees = 1000, 
                     interaction.depth = depth, 
                     shrinkage = lambda)
    
    probabilities <- predict(gbm_model, newdata = task1_test, n.trees = 1000, type = "response")
    
    predictions <- rep(0, nrow(task1_test))
    predictions <- ifelse(probabilities > .5, 1, 0)
  
    conf_matrix <- table(predictions, task1_test$label)
    
    gbdt_metrics <- rbind(gbdt_metrics, eval_metrics(paste("lambda:", as.character(lambda), ";", 
                                                           "depth:", as.character(depth)), conf_matrix))
    print(depth)
    print(lambda)
  }
}

gbdt_metrics$index <- 1:nrow(gbdt_metrics)

ggplot(gbdt_metrics[2:nrow(gbdt_metrics),], aes(x=index, F1)) + 
  geom_line() +
  geom_point() +
  geom_hline(yintercept = gbdt_metrics$F1[1], linetype = "solid", color = "black") + 
  annotate("text", x=1.75, y=gbdt_metrics$F1[1]+.0075, label = "Base Model") +
  geom_vline(xintercept = which.max(gbdt_metrics$F1), linetype = "dashed", color = "red") + 
  xlim(1,nrow(gbdt_metrics)) + 
  xlab("Setup Index") + 
  ylab("F1 Score") +
  ggtitle("Selecting Shrinkage and Depth to Maximize F1 Score") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 


```


```{r gbdt parameters}
paste("Optimal setup for our Gradient Boosted Decision Tress is", gbdt_metrics[which.max(gbdt_metrics$F1),1])
```


```{r gbdt threshold selection}

# Fit model with ideal hyperparameters on training data

tuned_gbdt <- gbm(label ~ ., data = task1_train, 
                     distribution = "bernoulli", 
                     n.trees = 1000, 
                     interaction.depth = 5, 
                     shrinkage = .05)

# Predict on the Validation Set
gbdt_validation_preds <- predict(tuned_gbdt, newdata = task1_validation[2:ncol(task1_validation)], n.trees = 1000, type = "response") # $predictions

# Calculate PR Curves Using Validation Labels (y_val)
gbdt_validation_pr_curve <- pr.curve(scores.class0 = gbdt_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)


cat("PR AUC (GBDT, Validation Set):", gbdt_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
gbdt_curve_df <- data.frame(Model = "Gradient Boosted Decision Trees", 
                          Recall = gbdt_validation_pr_curve$curve[, 1], 
                          Precision = gbdt_validation_pr_curve$curve[, 2], 
                          Threshold = gbdt_validation_pr_curve$curve[, 3])

gbdt_validation_pr_curve |> plot()

```


```{r gbdt optimal threshold implementation}

# Compute F1 Scores and Find Optimal Thresholds
gbdt_curve_df <- gbdt_curve_df |> mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Avoid division by zero

gbdt_threshold <- gbdt_curve_df[which.max(gbdt_curve_df$F1),4]

tuned_gbdt_probs <- predict(tuned_gbdt, newdata = task1_test, n.trees = 1000, type = "response")

tuned_gbdt_preds <-  rep(0, nrow(task1_test))
tuned_gbdt_preds <- ifelse(tuned_gbdt_probs > gbdt_threshold, 1, 0)

tuned_gbdt_conf_matrix <- table(tuned_gbdt_preds, task1_test$label)

```



#### SVM

```{r pca svm cost tuning}

pca_svm_validation_preds <- predict(pca_svmfit, newdata = reduced_validation[1:ncol(reduced_validation)-1], probabilities = TRUE)
pca_svm_validation_conf_matrix <- table(pca_svm_validation_preds, reduced_validation$label)
pca_svm_metrics <- eval_metrics("Cost = 10 (Base + PCA)", pca_svm_validation_conf_matrix)

# set costs list to iterate through
costs <- c(.1, .25, .5, 1, 2.5, 5, 20)

# within loop, fit on training, predict and evaluate on validation
for (i in costs){
  
  pca_model <- svm(label ~ ., data = reduced_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = i)
  
  pca_predictions <- predict(pca_model, newdata = reduced_validation[1:ncol(reduced_validation)-1], probabilities = TRUE)
  pca_conf_matrix <- table(pca_predictions, reduced_validation$label)
  pca_svm_metrics <- rbind(pca_svm_metrics, eval_metrics(paste("Cost =", as.character(i)), pca_conf_matrix))

}
pca_svm_metrics

```

```{r SVM FINAL, echo=TRUE}

tuned_svm <- svm(label ~ ., data = reduced_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = 5)

tuned_svm_preds <- predict(tuned_svm, newdata = reduced_test[1:ncol(reduced_test)-1], probabilities = TRUE)
tuned_svm_conf_matrix <- table(tuned_svm_preds, reduced_test$label)

```

## Model Evaluation

```{r making summary tables}

# need to add ROC / AUC info

base_summary_table <- rbind(eval_metrics("LDA", lda_conf_matrix), 
                       c("QDA", rep(NA,3)),
                       eval_metrics("Logistic Regression", logistic_conf_matrix),
                       eval_metrics("1NN", knn_conf_matrix),
                       eval_metrics("Random Forest", rf_conf_matrix),
                       eval_metrics("GBDT", gbdt_conf_matrix),
                       eval_metrics("SVM", svm_conf_matrix))

pca_summary_table <- rbind(eval_metrics("LDA", pca_lda_conf_matrix), 
                       eval_metrics("QDA", pca_qda_conf_matrix),
                       eval_metrics("Logistic Regression", pca_logistic_conf_matrix),
                       eval_metrics("KNN", pca_knn_conf_matrix),
                       eval_metrics("Random Forest", pca_rf_conf_matrix),
                       eval_metrics("GBDT", pca_gbdt_conf_matrix),
                       eval_metrics("SVM", pca_svm_conf_matrix))


tuned_summary_table <- rbind(eval_metrics("9NN", pca_tuned_knn_conf_matrix),
                       eval_metrics("GBDT with Optimal Threshold", tuned_gbdt_conf_matrix), 
                       eval_metrics("SVM with Optimal Cost", tuned_svm_conf_matrix))


```

```{r base auc curves, echo=TRUE}


lda_roc_curve <- roc(task1_test$label, lda_pred |> as.numeric(), levels = c(0,1), direction = "<")
lda_auc <- auc(lda_roc_curve)

logistic_roc_curve <- roc(task1_test$label, logistic_probs |> as.numeric(), levels = c(0,1), direction = "<")
logisitc_auc <- auc(logistic_roc_curve)

# qda_roc_curve <- roc(task1_test$label, qda_pred |> as.numeric(), levels = c(0,1), direction = "<")
# auc(qda_roc_curve)

knn_roc_curve <- roc(task1_test$label, knn_preds[,2] |> as.numeric(), levels = c(0,1), direction = "<")
knn_auc <- auc(knn_roc_curve)

gbdt_roc_curve <- roc(task1_test$label, gbdt_probs |> as.numeric(), levels = c(0,1), direction = "<")
gbdt_auc <- auc(gbdt_roc_curve)

rf_roc_curve <- roc(task1_test$label, rf_preds |> as.numeric(), levels = c(0,1), direction = "<")
rf_auc <- auc(rf_roc_curve)

svm_roc_curve <- roc(task1_test$label, svm_predictions |> as.numeric(), levels = c(0,1), direction = "<")
svm_auc <- auc(svm_roc_curve)

base_summary_table$AUC <- c(lda_auc, NA, logisitc_auc, knn_auc, rf_auc, gbdt_auc,  svm_auc) |> round(digits = 3)
```

```{r pc auc curves}
pca_lda_roc_curve <- roc(reduced_test$label, pca_lda_pred |> as.numeric(), levels = c(0,1), direction = "<")
pca_lda_auc <- auc(pca_lda_roc_curve)

pca_logistic_roc_curve <- roc(task1_test$label, pca_logistic_probs |> as.numeric(), levels = c(0,1), direction = "<")
pca_logisitc_auc <- auc(pca_logistic_roc_curve)

pca_qda_roc_curve <- roc(task1_test$label, pca_qda_pred |> as.numeric(), levels = c(0,1), direction = "<")
pca_qda_auc <- auc(pca_qda_roc_curve)

pca_knn_roc_curve <- roc(task1_test$label, pca_knn_preds[,2] |> as.numeric(), levels = c(0,1), direction = "<")
pca_knn_auc <- auc(pca_knn_roc_curve)

pca_gbdt_roc_curve <- roc(task1_test$label, pca_gbdt_probs |> as.numeric(), levels = c(0,1), direction = "<")
pca_gbdt_auc <- auc(pca_gbdt_roc_curve)

pca_rf_roc_curve <- roc(task1_test$label, pca_rf_preds |> as.numeric(), levels = c(0,1), direction = "<")
pca_rf_auc <- auc(pca_rf_roc_curve)

pca_svm_roc_curve <- roc(task1_test$label, pca_svm_predictions |> as.numeric(), levels = c(0,1), direction = "<")
pca_svm_auc <- auc(pca_svm_roc_curve)


pca_summary_table$AUC <- c(pca_lda_auc, pca_qda_auc, pca_logisitc_auc,  pca_knn_auc, pca_rf_auc, pca_gbdt_auc, pca_svm_auc) |> round(digits = 3)
```

```{r tuned auc curves}
pca_knn_roc_curve <- roc(reduced_test$label, pca_tuned_knn |> as.numeric(), levels = c(0,1), direction = "<")
pca_tuned_knn_auc <- auc(pca_knn_roc_curve)

tuned_gbdt_roc_curve <- roc(reduced_test$label, tuned_gbdt_preds |> as.numeric(), levels = c(0,1), direction = "<")
tuned_gbdt_auc <- auc(tuned_gbdt_roc_curve)

tuned_svm_roc_curve <- roc(reduced_test$label, tuned_svm_preds |> as.numeric(), levels = c(0,1), direction = "<")
tuned_svm_auc <- auc(tuned_svm_roc_curve)

tuned_summary_table$AUC <- c(pca_tuned_knn_auc, tuned_gbdt_auc, tuned_svm_auc) |> round(digits = 3)

```


```{r}
base_summary_table

pca_summary_table

tuned_summary_table
```

## T1.4

### mypredict()

```{r function implementation}

mypredict <- function(){
  
  ### function takes nothing as input
  ###
  ### returns predictions, one per row, in the same order as test.csv.gz
  
  # shuffling data 
  data <- read.csv("/Users/finbarrhodes/Documents/ST443/Final Project/data1.csv.gz") # test.csv.gz
  data$label <- if_else(data$label == "TREG", 1, 0)
  
  # predict on top model from T1.3
  probabilities <- predict(tuned_gbdt, newdata = data[,-1], n.trees = 1000, type = "response")
  predictions <- rep(0, nrow(data))
  predictions <- ifelse(probabilities > gbdt_threshold, 1, 0) # threshold defined as 0.4698173
  
  write.table(predictions |> as.data.frame(), 
                         file = "predictions.txt", 
                         quote = FALSE, 
                         row.names = FALSE, 
                         col.names = FALSE)
  
}


```







