---
title: "TASK2"
output: html_document
date: "2024-11-13"
---

--- LIBRARIES ---

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
```

```{r}
data <- read.csv("data2.csv.gz")
```

Binding: Indicates that the compound interacts or attaches effectively to the thrombin target site.(1)
Non-binding: Indicates that the compound does not interact effectively with thrombin.(0)
In the context of drug discovery, thrombin is a protein that could be a target for drugs aimed at affecting blood clotting, and identifying compounds that bind to thrombin can be a critical step in developing new medications.

```{r}
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)

```

```{r}
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
  if (data[i, 1] == -1) {
    count <- count + 1
  }
}

binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")

```



```{r}
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")

```

```{r}
head(data)
```

```{r}
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
```


```{r}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
```

To compare our models we mesure their performance on unseen data. So we split the dataset in training and test
```{r}
# Calculate row indices for each split
set.seed(12)  # Set seed for reproducibility
unique_data <- unique_data[sample(nrow(unique_data)), ]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 10% for validation

# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data



# # Use 80% of the data for training
# end_training <- floor(nrow(unique_data) * 0.8)
# # Split the data into training and testing sets
# training_data <- unique_data[1:end_training, ]
# testing_data <- unique_data[(end_training + 81):nrow(unique_data), ]
print(sum(training_data[,1]))
print(sum(testing_data[,1]))
print(sum(validation_data[,1]))

```


Lets build Lasso and Elastic Net with the training data
```{r}
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_test <- testing_data[,1]

#create training and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
```


```{r}
#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
```


```{r}
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
```
Plot the choice of lamda
```{r}
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
```
I commented the ridge regression out of the code because it keeps all the features and just makes the coefficeints smaller while the other models actually removes features by shrinking they coefficeints to 0.

Now we us the models on unseen data to predict the labels and evaluate the performance
Because we have unbalanced data instead of using the normal ROC and AUC we use the PR ROC and PR AUC
```{r} 

library(PRROC)
# Calculate PR AUC for predictions_1d (Lasso)
predictions_1d <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response") #unseen data prediction
pr_1d <- pr.curve(scores.class0 = predictions_1d, weights.class0 = y_test, curve = TRUE)
cat("PR AUC (Lasso):", pr_1d$auc.integral, "\n")

# Calculate PR AUC for predictions_3d (Elastic Net)
predictions_3d <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")
pr_3d <- pr.curve(scores.class0 = predictions_3d, weights.class0 = y_test, curve = TRUE)
cat("PR AUC (Elastic Net):", pr_3d$auc.integral, "\n")


df_1d <- data.frame(Recall = pr_1d$curve[, 1], Precision = pr_1d$curve[, 2], Threshold = pr_1d$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d$curve[, 1], Precision = pr_3d$curve[, 2], Threshold = pr_3d$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)


# Compute F1 scores for each threshold to find the optimal
pr_data <- pr_data %>%
  mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Add small value to avoid division by zero

# Find optimal threshold for each model (maximizing F1 score)
optimal_thresholds <- pr_data %>%
  group_by(Model) %>%
  filter(F1 == max(F1)) %>%
  summarize(Optimal_Threshold = Threshold[1], Max_F1 = F1[1])

# Print optimal thresholds
print(optimal_thresholds)

# Plot Precision-Recall Curves with ggplot2
ggplot(pr_data, aes(x = Recall, y = Precision, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "Precision-Recall Curves for Lasso and Elastic Net",
    x = "Recall",
    y = "Precision",
    color = "Model"
  ) +
  theme_minimal()
```


```{r}
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning thew parameters)
predicted_classes_lasso <- ifelse(predictions_1d > 0.4048683, 1, 0)
accuracy <- mean(predicted_classes_lasso == y_true)
cat("The accuracy of Lasso on the test set is:", accuracy, "\n")

predicted_classes_EN <- ifelse(predictions_3d > 0.4200326, 1, 0) # 
accuracy <- mean(predicted_classes_EN == y_true)
cat("The accuracy of Elastic Net on the test set is:", accuracy, "\n")

```
We tuned the parameters of each moddel such that it uses thye optimal threshold and we obatain the same accracy but 
we get abigger AUC 

We can test on how well the 2 model perform top choose the best: 
```{r}
# Cross validation errors when we training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)

cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
```

```{r}
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_test)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)

conf_matrix_EN <- table(Predicted =  predicted_classes_EN , Actual = y_test)
print("Confusion Matrix Elastic:")
print(conf_matrix_EN)
```


################ XGBOOST  ##########################
FIRST STEP: Create train and test data.

BASED ON HOW DATA IS SPLITED EARLIER, TO KEEP THE SAME y_test


---- CODIGO CAMBIADO ----

```{r}
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]

#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")

```



```{r}
library(xgboost)
library(Matrix)
library(glmnet)
library(caTools)
# Convert to DMatrix format for XGBoost

dtrain_xg <- xgb.DMatrix(data = sparse_data_train, label = y_train)
dvalid_xg <- xgb.DMatrix(data = sparse_data_valid, label = y_valid)
dtest_xg <- xgb.DMatrix(data = sparse_data_test, label = y_test)

```


```{r}
# Simplified grid for essential parameters
param_grid_xg <- expand.grid(
  max_depth = c(4, 6, 8), # tree depth
  eta = c(0.1, 0.3), # Learning rate F_t+1 = eta * T_t + F_t 
  subsample = c(0.6, 0.8), # fraction of rows
  colsample_bytree = c(0.8) # fraction of features
)

# Track results
best_model_xg <- NULL
best_logloss_xg <- Inf

for (i in 1:nrow(param_grid_xg)) {
  params_xg <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = param_grid_xg$max_depth[i],
    eta = param_grid_xg$eta[i],
    subsample = param_grid_xg$subsample[i],
    colsample_bytree = param_grid_xg$colsample_bytree[i]
  )
  
  # Cross-validation for current parameters
  cv_xg <- xgb.cv(
    params = params_xg,
    data = dtrain_xg,
    nrounds = 50,              # Keep this small to limit computation
    nfold = 5,                 # Stratified 5-fold CV
    early_stopping_rounds = 10,
    verbose = FALSE
  )
  
  # Update the best model if logloss improves
  min_logloss_xg <- min(cv_xg$evaluation_log$test_logloss_mean)
  if (min_logloss_xg < best_logloss_xg) {
    best_logloss_xg <- min_logloss_xg
    best_model_xg <- params_xg
  }
}
# STORE BEST_MODEL PARAMETERS
```


```{r}
# Initialize a data frame to store results
if (!exists("model_results_xg")) {
  model_results_xg <- data.frame(
    eta = numeric(),
    max_depth = integer(),
    subsample = numeric(),
    colsample_bytree = numeric(),
    logloss = numeric(),
    stringsAsFactors = FALSE
  )
}

# Append the best_model parameters and log loss
model_results_xg <- rbind(
  model_results_xg,
  data.frame(
    eta = best_model_xg$eta,
    max_depth = best_model_xg$max_depth,
    subsample = best_model_xg$subsample,
    colsample_bytree = best_model_xg$colsample_bytree,
    logloss = best_logloss_xg  # Use the corresponding logloss for the best model
  )
)

# Print the updated table
print(model_results_xg)

print(best_model_xg)

```


```{r}
final_params_xg <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "logloss",
  eta = best_model_xg$eta,                 # Replace with the optimal eta value
  max_depth = best_model_xg$max_depth,     # Replace with the optimal max_depth
  subsample = best_model_xg$subsample,     # Replace with the optimal subsample value
  colsample_bytree = best_model_xg$colsample_bytree # Replace with optimal colsample_bytree
)

# Train the model with the optimal parameters
final_model_xg <- xgb.train(
  params = final_params_xg, 
  data = dtrain_xg, 
  nrounds = 100,                        # You can adjust the number of rounds based on early stopping results
  watchlist = list(train = dtrain_xg), 
  early_stopping_rounds = 10,           # Stop training if no improvement is seen in 10 rounds
  verbose = 0                           # Optional: set to 0 for silent training
)

# Make predictions on the test data
predictions_xg_train <- predict(final_model_xg, dtrain_xg) 

# Convert probabilities to binary predictions (0 or 1)
predictions_binary_xg_train <- ifelse(predictions_xg_train > 0.5, 1, 0)

# Evaluate the final model
accuracy_xg_train <- sum(predictions_binary_xg_train == y_train) / length(y_train)
print(paste("Final Accuracy for Train data with 0.5:", accuracy_xg_train))

# Confusion matrix
conf_matrix_xg_train <- table(Predicted = predictions_binary_xg_train, Actual = y_train)
print("Confusion Matrix:")
print(conf_matrix_xg_train)

# Feature importance
importance_matrix_xg_train <- xgb.importance(model = final_model_xg)
print(importance_matrix_xg_train)

top_features_xg <- importance_matrix_xg_train[1:25, ]  # Select the top 30 features

# Plot the top 30 features
xgb.plot.importance(importance_matrix = top_features_xg, 
                    main = "Top 30 Most Important Features",
                    xlab = "Feature Importance")
```



```{r}
library(ggplot2)

# Initialize an empty data frame to store results
predictions_xg_valid <- predict(final_model_xg, dvalid_xg) 
threshold_results_xg_valid <- data.frame(Threshold_xg = numeric(), BalancedAccuracy_xg = numeric())

# Define a sequence of threshold values
threshold_values_xg_valid <- seq(0, 1, by = 0.01)

# Loop through each threshold value
for (threshold_xg in threshold_values_xg_valid) {
  # Convert probabilities to binary predictions based on the current threshold
  predicted_classes_xg_valid <- ifelse(predictions_xg_valid > threshold_xg, 1, 0)
  
  # Calculate the confusion matrix
  conf_matrix_xg_valid <- table(Predicted_xg = predicted_classes_xg_valid, Actual_xg = y_valid)
  
  # Extract TP, TN, FP, FN (use tryCatch for error handling in case of missing categories)
  tn_xg <- tryCatch(conf_matrix_xg_valid[1, 1], error = function(e) 0)
  tp_xg <- tryCatch(conf_matrix_xg_valid[2, 2], error = function(e) 0)
  fp_xg <- tryCatch(conf_matrix_xg_valid[2, 1], error = function(e) 0)
  fn_xg <- tryCatch(conf_matrix_xg_valid[1, 2], error = function(e) 0)
  
  # Calculate balanced accuracy
  balan_acc_xg_valid <- 0.5 * (tp_xg / (tp_xg + fn_xg + 1e-6) + tn_xg / (tn_xg + fp_xg + 1e-6)) # Add small value to avoid division by zero
  
  # Append the results to the data frame
  threshold_results_xg <- rbind(threshold_results_xg, data.frame(Threshold_xg = threshold_xg, BalancedAccuracy_xg = balan_acc_xg_valid))
}

# Print the results
# print(threshold_results_xg)

# Plot Balanced Accuracy vs Threshold
ggplot(threshold_results_xg, aes(x = Threshold_xg, y = BalancedAccuracy_xg)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Balanced Accuracy vs Threshold (XGBoost)",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

max_balanced_accuracy_xg <- threshold_results_xg[which.max(threshold_results_xg$BalancedAccuracy_xg), ]

# Output the results
cat("Threshold with Maximum Balanced Accuracy (XGBoost):\n")
cat("Threshold_xg:", max_balanced_accuracy_xg$Threshold_xg, "\n")
cat("Balanced Accuracy_xg:", max_balanced_accuracy_xg$BalancedAccuracy_xg, "\n")

```



```{r}
# BEST THRESHOLD BY VALIDATING
predictions_xg_test <- predict(final_model_xg, dtest_xg) 
predicted_classes_xg_test <- ifelse(predictions_xg_test > max_balanced_accuracy_xg$Threshold_xg , 1, 0)
# TO TEST A PARTICULAR THRESHOLD:
# predicted_classes_xg_test <- ifelse(predictions_xg_test > 0.5 , 1, 0)

conf_matrix_xg_test <- table(Predicted = predicted_classes_xg_test, Actual = y_test)
print("Confusion Matrix Test XG:")
print(conf_matrix_xg_test)

tn_xg_t = conf_matrix_xg_test[1,1]
tp_xg_t = conf_matrix_xg_test[2,2]
fp_xg_t = conf_matrix_xg_test[2,1]
fn_xg_t = conf_matrix_xg_test[1,2]

balan_acc_xg_test = 0.5*( tp_xg_t / (tp_xg_t+fn_xg_t) + tn_xg_t / (tn_xg_t + fp_xg_t) )
print(balan_acc_xg_test)

```
Without validation, using Threshold = 0.5
[1] "Confusion Matrix Test XG:"
         Actual
Predicted   0   1
        0 112   3
        1   0   5
[1] Balanced accuracy for Test 0.8125
------------------------------------------
With validation, using Threshold = 0.039
[1] "Confusion Matrix Test XG:"
         Actual
Predicted   0   1
        0 107   1
        1   5   7
[1] Balanced accuracy for Test  0.9151786


----- SUPPORT VECTOR MACHINES -----

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
data <- read.csv("data2.csv.gz")
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
# Calculate row indices for each split
set.seed(12)  # Set seed for reproducibility
unique_data <- unique_data[sample(nrow(unique_data)), ]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 10% for validation

# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
```


```{r}
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]

#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")


```



```{r}
# Install necessary packages (if not already installed)
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")

# Load libraries
library(LiblineaR)
library(Matrix)

# y_train <- as.factor(y_train)  # Convert to factor if it's classification

tryTypes=c(1,2,3,4,6)
tryCosts=c(1000, 100, 1, 0.1, 0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
  for(co in tryCosts){
  acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=0,cross=5,verbose=FALSE)

  if(acc>bestAcc){
    bestCost=co
    bestAcc=acc
    bestType=ty
  }
  }
}

cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")
```

THIS CODE PICKS AN AMOUNT OF FEATURES





```{r}
#DATA
set.seed(12)
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")

# -----------
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")

# Load libraries
library(SparseM)
library(LiblineaR)
library(Matrix)

# y_train <- as.factor(y_train)  # Convert to factor if it's classification

tryTypes=c(1,2,3,4,6)
tryCosts=c(1000, 100, 10 , 1, 0.1, 0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
  for(co in tryCosts){
  acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=0,cross=5,verbose=FALSE)

  if(acc>bestAcc){
    bestCost=co
    bestAcc=acc
    bestType=ty
  }
  }
}

cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")


# -----------
#First model
bestType <- 6
final_model_svm <- LiblineaR( data=sparse_data_train , target=y_train ,type=bestType,cost=bestCost,bias=0,verbose=FALSE)
weights_svm <- round(final_model_svm$W, digits = 6)
#Pick features
feature_impor_svm <- abs(weights_svm)
feature_rank_svm <- order(feature_impor_svm, decreasing = T)

#First model
k_fea <- 100
# Initialize variables
k_values <- seq(200, 250, by = 10)  # Test k_fea values from 100 to 1000 in steps of 100
max_balanced_acc <- -Inf             # To store the maximum balanced accuracy
best_k_fea <- NULL                   # To store the best k_fea
df_data_train <- as.data.frame(as.matrix(sparse_data_train))

# Iterate over different values of k_fea
for (k_fea in k_values) {
  # Step 1: Select the top k features
  top_k_fea <- feature_rank_svm[1:k_fea]
  top_k_wei <- weights_svm[top_k_fea]
  
  # Step 2: Change data for the model

  df_data_train_selected <- df_data_train[, top_k_fea, drop = FALSE]
  sparse_data_train_svm_red <- as(as.matrix(df_data_train_selected), "dgCMatrix")
  
  # Step 3: Train the model
  final_model_svm_red <- LiblineaR(
    data = sparse_data_train_svm_red,
    target = y_train,
    type = 6,
    cost = bestCost,
    bias = 0,
    verbose = FALSE
  )
  
  # Step 4: Make predictions on the validation data
  predictions_svm_valid_list <- predict( final_model_svm_red , sparse_data_valid, proba = TRUE)
  
  # Step 5: Compute the confusion matrix
  conf_matrix_svm_valid <- table( Predicted = predictions_svm_valid_list$predictions , Actual = y_valid )
  
  tn_svm_v <- conf_matrix_svm_valid[1, 1]
  tp_svm_v <- conf_matrix_svm_valid[2, 2]
  fp_svm_v <- conf_matrix_svm_valid[2, 1]
  fn_svm_v <- conf_matrix_svm_valid[1, 2]
  
  # Step 6: Compute the balanced accuracy
  balan_acc_svm_valid <- 0.5 * (tp_svm_v / (tp_svm_v + fn_svm_v) + tn_svm_v / (tn_svm_v + fp_svm_v))
  
  # Step 7: Update the maximum balanced accuracy and best k_fea
  if (balan_acc_svm_valid > max_balanced_acc) {
    max_balanced_acc <- balan_acc_svm_valid
    best_k_fea <- k_fea
  }
  
  # Print the current k_fea and its balanced accuracy
  cat("k_fea:", k_fea, "Balanced Accuracy:", balan_acc_svm_valid, "\n")
}

# Output the best k_fea and corresponding maximum balanced accuracy
cat("Best k_fea:", best_k_fea, "with Balanced Accuracy:", max_balanced_acc, "\n")
  
# ------------- Given the defined k, calculates

k_fea <- best_k_fea
top_k_fea <- feature_rank_svm[1:k_fea]
top_k_wei <- weights_svm[top_k_fea]
  
  # Step 2: Change data for the model

df_data_train_selected <- df_data_train[, top_k_fea, drop = FALSE]
sparse_data_train_svm_red <- as(as.matrix(df_data_train_selected), "dgCMatrix")
  
  # Step 3: Train the model
final_model_svm_red_sel <- LiblineaR(
  data = sparse_data_train_svm_red,
  target = y_train,
  type = 6,
  cost = bestCost,
  bias = 0,
  verbose = FALSE
)
  
  # Step 4: Make predictions on the validation data
predictions_svm_valid_list_k <- predict( final_model_svm_red_sel , sparse_data_valid, proba = TRUE)
  
  # Step 5: Compute the confusion matrix
conf_matrix_svm_valid_k <- table( Predicted = predictions_svm_valid_list_k$predictions , Actual = y_valid )
  
tn_svm_v <- conf_matrix_svm_valid_k[1, 1]
tp_svm_v <- conf_matrix_svm_valid_k[2, 2]
fp_svm_v <- conf_matrix_svm_valid_k[2, 1]
fn_svm_v <- conf_matrix_svm_valid_k[1, 2]
  
  # Step 6: Compute the balanced accuracy
balan_acc_svm_valid <- 0.5 * (tp_svm_v / (tp_svm_v + fn_svm_v) + tn_svm_v / (tn_svm_v + fp_svm_v))

cat("k_fea:", k_fea, "Balanced Accuracy:", balan_acc_svm_valid, "\n")
print(conf_matrix_svm_valid)

#----------------- DETERMINE OPTIMAL THRESHOLD with Y_VALID
library(ggplot2)

# Initialize an empty data frame to store results
threshold_results_svm <- data.frame(Threshold_svm = numeric(), BalancedAccuracy_svm = numeric())

# Define a sequence of threshold values
threshold_values_svm <- seq(0,1, by = 0.001)
final_model_svm_red <- LiblineaR(data=sparse_data_train_svm_red,target=y_train,type=bestType,cost=bestCost,bias=0,verbose=FALSE)
predictions_svm_valid_list <- predict(final_model_svm_red, sparse_data_valid, proba = T) #CHANGE TEST INTO VALIDATE
predictions_svm_valid <- predictions_svm_valid_list$probabilities[, 2]

# Loop through each threshold value
for (threshold_svm in threshold_values_svm) {
  # Convert probabilities to binary predictions based on the current threshold
  predicted_classes_svm <- ifelse(predictions_svm_valid > threshold_svm, 1, 0)
  
  # Calculate the confusion matrix
  conf_matrix_svm <- table(Predicted_svm = predicted_classes_svm, Actual_xg = y_valid) #WHAT DATA Y
  
  # Extract TP, TN, FP, FN (use tryCatch for error handling in case of missing categories)
  tn_svm <- tryCatch(conf_matrix_svm[1, 1], error = function(e) 0)
  tp_svm <- tryCatch(conf_matrix_svm[2, 2], error = function(e) 0)
  fp_svm <- tryCatch(conf_matrix_svm[2, 1], error = function(e) 0)
  fn_svm <- tryCatch(conf_matrix_svm[1, 2], error = function(e) 0)
  
  # Calculate balanced accuracy
  balan_acc_svm <- 0.5 * (tp_svm / (tp_svm + fn_svm + 1e-6) + tn_svm / (tn_svm + fp_svm + 1e-6)) # Add small value to avoid division 0
  
  # Append the results to the data frame
  threshold_results_svm <- rbind(threshold_results_svm, data.frame(Threshold_svm = threshold_svm, BalancedAccuracy_svm = balan_acc_svm))
}

# Print the results
# print(threshold_results_xg)

# Plot Balanced Accuracy vs Threshold
ggplot(threshold_results_svm, aes(x = Threshold_svm, y = BalancedAccuracy_svm)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Balanced Accuracy vs Threshold with Maximum Balanced Accuracy (SVM with", k_fea, " features)",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

max_balanced_accuracy_svm <- threshold_results_svm[which.max(threshold_results_svm$BalancedAccuracy_svm), ]

# Output the results
cat("Threshold with Maximum Balanced Accuracy (SVM with", k_fea, " features):\n")
cat("Threshold_SVM:", max_balanced_accuracy_svm$Threshold_svm, "\n")
cat("Balanced Accuracy_SVM_for best Threshold in y_valid:", max_balanced_accuracy_svm$BalancedAccuracy_svm, "\n")

# --------------------------------------------
predictions_svm_test_list <- predict(final_model_svm_red, sparse_data_test, proba = T) #CHANGE TEST INTO VALIDATE
predictions_svm_test <- predictions_svm_test_list$probabilities[, 2]


predicted_classes_svm_test <- ifelse(predictions_svm_test > max_balanced_accuracy_svm$Threshold_svm , 1, 0)
# TO TEST A PARTICULAR THRESHOLD:
# predicted_classes_xg_test <- ifelse(predictions_xg > 0.5 , 1, 0)

conf_matrix_svm_test <- table(Predicted = predicted_classes_svm_test, Actual = y_test)
print("Confusion Matrix Test SVM:")
print(conf_matrix_svm_test)

tn_svm_t = conf_matrix_svm_test[1,1]
tp_svm_t = conf_matrix_svm_test[2,2]
fp_svm_t = conf_matrix_svm_test[2,1]
fn_svm_t = conf_matrix_svm_test[1,2]

balan_acc_svm_test = 0.5*( tp_svm_t / (tp_svm_t+fn_svm_t) + tn_svm_t / (tn_svm_t + fp_svm_t) )
print(balan_acc_svm_test)


```
k_fea: 200 Balanced Accuracy: 0.5491071 
k_fea: 210 Balanced Accuracy: 0.5446429 
k_fea: 220 Balanced Accuracy: 0.6160714 
k_fea: 230 Balanced Accuracy: 0.5491071 
k_fea: 240 Balanced Accuracy: 0.6785714 
k_fea: 250 Balanced Accuracy: 0.5535714 
k_fea: 260 Balanced Accuracy: 0.4955357 
k_fea: 270 Balanced Accuracy: 0.5535714 
k_fea: 280 Balanced Accuracy: 0.6785714 
k_fea: 290 Balanced Accuracy: 0.6785714 
k_fea: 300 Balanced Accuracy: 0.6160714 
Best k_fea: 240 with Balanced Accuracy: 0.6785714 


```{r}
set.seed(123)
k_fea <- best_k_fea
top_k_fea <- feature_rank_svm[1:k_fea]
top_k_wei <- weights_svm[top_k_fea]
  
  # Step 2: Change data for the model
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, top_k_fea, drop = FALSE]
sparse_data_train_svm_red <- as(as.matrix(df_data_train_selected), "dgCMatrix")
  
  # Step 3: Train the model
final_model_svm_red <- LiblineaR(
  data = sparse_data_train_svm_red,
  target = y_train,
  type = 6,
  cost = 100,
  bias = 0,
  verbose = FALSE
)
  
  # Step 4: Make predictions on the validation data
predictions_svm_valid_list <- predict( final_model_svm_red , sparse_data_valid, proba = TRUE)
  
  # Step 5: Compute the confusion matrix
conf_matrix_svm_valid <- table( Predicted = predictions_svm_valid_list$predictions , Actual = y_valid )
  
tn_svm_v <- conf_matrix_svm_valid[1, 1]
tp_svm_v <- conf_matrix_svm_valid[2, 2]
fp_svm_v <- conf_matrix_svm_valid[2, 1]
fn_svm_v <- conf_matrix_svm_valid[1, 2]
  
  # Step 6: Compute the balanced accuracy
balan_acc_svm_valid <- 0.5 * (tp_svm_v / (tp_svm_v + fn_svm_v) + tn_svm_v / (tn_svm_v + fp_svm_v))

cat("k_fea:", k_fea, "Balanced Accuracy:", balan_acc_svm_valid, "\n")
print(conf_matrix_svm_valid)

```



```{r}
set.seed(123)
#DATA
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")

#First model
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train, type=6,cost=100,bias=0,verbose=FALSE)
weights_svm <- final_model_svm$W
#Pick features
feature_impor_svm <- abs(weights_svm)
feature_rank_svm <- order(feature_impor_svm, decreasing = T)
k_fea <- 240

top_k_fea <- feature_rank_svm[1:k_fea]
top_k_wei <- weights_svm[top_k_fea]
  
  # Step 2: Change data for the model
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, top_k_fea, drop = FALSE]
sparse_data_train_svm_red <- as(as.matrix(df_data_train_selected), "dgCMatrix")


```

THIS CHUNK GIVES Acc GIVEN ON Y_VALID WITH ALL FEATURES AND CUSTOM THRESHOLD
```{r}
set.seed(123)
# Make predictions on the valid data
final_model_svm_red <- LiblineaR(data=sparse_data_train_svm_red,target=y_train,type=bestType,cost=bestCost,bias=0,verbose=FALSE)
predictions_svm_valid_list <- predict(final_model_svm_red, sparse_data_valid, proba = T)

conf_matrix_svm_valid <- table(Predicted = predictions_svm_valid_list$predictions, Actual = y_valid)
print("Confusion Matrix Valid SVM:")
print(conf_matrix_svm_valid)

tn_svm_v = conf_matrix_svm_valid[1,1]
tp_svm_v = conf_matrix_svm_valid[2,2]
fp_svm_v = conf_matrix_svm_valid[2,1]
fn_svm_v = conf_matrix_svm_valid[1,2]

balan_acc_svm_valid = 0.5*( tp_svm_v / (tp_svm_v+fn_svm_v) + tn_svm_v / (tn_svm_v + fp_svm_v) )
print(balan_acc_svm_valid)

```


 THIS PART GIVES THE BEST THRESHOLD TO VALID
```{r}
library(ggplot2)

# Initialize an empty data frame to store results
threshold_results_svm <- data.frame(Threshold_svm = numeric(), BalancedAccuracy_svm = numeric())

# Define a sequence of threshold values
threshold_values_svm <- seq(0,1, by = 0.001)
final_model_svm_red <- LiblineaR(data=sparse_data_train_svm_red,target=y_train,type=bestType,cost=bestCost,bias=0,verbose=FALSE)
predictions_svm_valid_list <- predict(final_model_svm_red, sparse_data_valid, proba = T) #CHANGE TEST INTO VALIDATE
predictions_svm_valid <- predictions_svm_valid_list$probabilities[, 2]

# Loop through each threshold value
for (threshold_svm in threshold_values_svm) {
  # Convert probabilities to binary predictions based on the current threshold
  predicted_classes_svm <- ifelse(predictions_svm_valid > threshold_svm, 1, 0)
  
  # Calculate the confusion matrix
  conf_matrix_svm <- table(Predicted_svm = predicted_classes_svm, Actual_xg = y_valid) #WHAT DATA Y
  
  # Extract TP, TN, FP, FN (use tryCatch for error handling in case of missing categories)
  tn_svm <- tryCatch(conf_matrix_svm[1, 1], error = function(e) 0)
  tp_svm <- tryCatch(conf_matrix_svm[2, 2], error = function(e) 0)
  fp_svm <- tryCatch(conf_matrix_svm[2, 1], error = function(e) 0)
  fn_svm <- tryCatch(conf_matrix_svm[1, 2], error = function(e) 0)
  
  # Calculate balanced accuracy
  balan_acc_svm <- 0.5 * (tp_svm / (tp_svm + fn_svm + 1e-6) + tn_svm / (tn_svm + fp_svm + 1e-6)) # Add small value to avoid division 0
  
  # Append the results to the data frame
  threshold_results_svm <- rbind(threshold_results_svm, data.frame(Threshold_svm = threshold_svm, BalancedAccuracy_svm = balan_acc_svm))
}

# Print the results
# print(threshold_results_xg)

# Plot Balanced Accuracy vs Threshold
ggplot(threshold_results_svm, aes(x = Threshold_svm, y = BalancedAccuracy_svm)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Balanced Accuracy vs Threshold with Maximum Balanced Accuracy (SVM with", k_fea, " features)",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

max_balanced_accuracy_svm <- threshold_results_svm[which.max(threshold_results_svm$BalancedAccuracy_svm), ]

# Output the results
cat("Threshold with Maximum Balanced Accuracy (SVM with", k_fea, " features):\n")
cat("Threshold_SVM:", max_balanced_accuracy_svm$Threshold_svm, "\n")
cat("Balanced Accuracy_SVM:", max_balanced_accuracy_svm$BalancedAccuracy_svm, "\n")

```


THIS CODE GIVES THE FINAL BALANCED ACCURACY TO TEST
USING K FEATURES AND HAVING TUNED 
```{r}
predictions_svm_test_list <- predict(final_model_svm_red, sparse_data_test, proba = T) #CHANGE TEST INTO VALIDATE
predictions_svm_test <- predictions_svm_test_list$probabilities[, 2]


predicted_classes_svm_test <- ifelse(predictions_svm_test > max_balanced_accuracy_svm$Threshold_svm , 1, 0)
# TO TEST A PARTICULAR THRESHOLD:
# predicted_classes_xg_test <- ifelse(predictions_xg > 0.5 , 1, 0)

conf_matrix_svm_test <- table(Predicted = predicted_classes_svm_test, Actual = y_test)
print("Confusion Matrix Test SVM:")
print(conf_matrix_svm_test)

tn_svm_t = conf_matrix_svm_test[1,1]
tp_svm_t = conf_matrix_svm_test[2,2]
fp_svm_t = conf_matrix_svm_test[2,1]
fn_svm_t = conf_matrix_svm_test[1,2]

balan_acc_svm_test = 0.5*( tp_svm_t / (tp_svm_t+fn_svm_t) + tn_svm_t / (tn_svm_t + fp_svm_t) )
print(balan_acc_svm_test)

```

THIS PART IS LIKE WHAT HAPPENDS WITH THE PREDICTIONS AS A POSSIBLE OUTPUT OF 

```{r}
predictions_svm_test_list2 <- predict(final_model_svm_red, sparse_data_test) #CHANGE TEST INTO VALIDATE

conf_matrix_svm_test2 <- table(Predicted = predictions_svm_test_list2$predictions, Actual = y_test)
print("Confusion Matrix Test SVM:")
print(conf_matrix_svm_test2)

tn_svm_t = conf_matrix_svm_test2[1,1]
tp_svm_t = conf_matrix_svm_test2[2,2]
fp_svm_t = conf_matrix_svm_test2[2,1]
fn_svm_t = conf_matrix_svm_test2[1,2]

balan_acc_svm_test2 = 0.5*( tp_svm_t / (tp_svm_t+fn_svm_t) + tn_svm_t / (tn_svm_t + fp_svm_t) )
print(balan_acc_svm_test2)

```


```{r}
100 , 300 , 
0.537062 , 0.7345013 ,
```

