cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
# Extract values from the confusion matrix
TN_EN <- conf_matrix_EN[1, 1]
FP_EN <- conf_matrix_EN[2, 1]
FN_EN <- conf_matrix_EN[1, 2]
TP_EN <- conf_matrix_EN[2, 2]
# Calculate Balanced Accuracy for Elastic Net
sensitivity_EN <- TP_EN / (TP_EN + FN_EN)  # Sensitivity = TP / (TP + FN)
specificity_EN <- TN_EN / (TN_EN + FP_EN)  # Specificity = TN / (TN + FP)
balanced_accuracy_EN <- (sensitivity_EN + specificity_EN) / 2
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
calculate_balanced_accuracy <- function(conf_matrix) {
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]
TP <- conf_matrix[2, 2]
sensitivity <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)  # Specificity
(sensitivity + specificity) / 2
}
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
balanced_accuracy_EN <- calculate_balanced_accuracy(conf_matrix_EN)
cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
conf_matrix_RF <- table(Predicted =  predicted_classes_RF , Actual = y_true)
set.seed(123)
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_val <- validation_data[ ,1]
y_true <- testing_data[,1]
#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_val <- validation_data[,-1]
sparse_data_val <- as(features_val, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
predicted_classes_RF <- ifelse(predictions_rf > 0.2207650, 1, 0) #Using optimal threshold
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
conf_matrix_RF <- table(Predicted =  predicted_classes_RF , Actual = y_true)
length(predicted_classes_RF)
length(y_true)
# Calculate row indices for each split
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 15% for validation
# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
# Print dimensions of each set
print(dim(training_data))
print(dim(validation_data))
print(dim(testing_data))
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
data <- read.csv("data2.csv.gz")
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
if (data[i, 1] == -1) {
count <- count + 1
}
}
binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")
head(data)
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
if (data[i, 1] == -1) {
data[i, 1] <- 0
} else {
data[i, 1] <- 1
}
}
print(data)
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
# Calculate row indices for each split
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 15% for validation
# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
# Print dimensions of each set
print(dim(training_data))
print(dim(validation_data))
print(dim(testing_data))
set.seed(123)
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_val <- validation_data[ ,1]
y_true <- testing_data[,1]
#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_val <- validation_data[,-1]
sparse_data_val <- as(features_val, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
set.seed(123)
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
library(PRROC)
# Predict on the Validation Set
# Lasso Model Predictions
predictions_1d_val <- predict(cv_fit_1d, sparse_data_val, s = "lambda.min", type = "response")
# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_val <- predict(cv_fit_3d, sparse_data_val, s = "lambda.min", type = "response")
# Calculate PR Curves Using Validation Labels (y_val)
# PR Curve for Lasso
pr_1d_val <- pr.curve(scores.class0 = predictions_1d_val, weights.class0 = y_val, curve = TRUE)
cat("PR AUC (Lasso, Validation Set):", pr_1d_val$auc.integral, "\n")
# PR Curve for Elastic Net
pr_3d_val <- pr.curve(scores.class0 = predictions_3d_val, weights.class0 = y_val, curve = TRUE)
cat("PR AUC (Elastic Net, Validation Set):", pr_3d_val$auc.integral, "\n")
# Create Data Frames for Precision-Recall Curves
df_1d <- data.frame(Recall = pr_1d_val$curve[, 1], Precision = pr_1d_val$curve[, 2], Threshold = pr_1d_val$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d_val$curve[, 1], Precision = pr_3d_val$curve[, 2], Threshold = pr_3d_val$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)
# Compute F1 Scores and Find Optimal Thresholds
pr_data <- pr_data %>%
mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Avoid division by zero
# Find the Optimal Threshold for Each Model
optimal_thresholds <- pr_data %>%
group_by(Model) %>%
filter(F1 == max(F1)) %>%
summarize(Optimal_Threshold = Threshold[1], Max_F1 = F1[1])
# Print Optimal Thresholds
print(optimal_thresholds)
library(PRROC)
# Predict on the test to see performance
# Lasso Model Predictions
predictions_1d_test <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response")
# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_test <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")
# Calculate PR Curves Using true Labels
# PR Curve for Lasso
pr_1d_test <- pr.curve(scores.class0 = predictions_1d_test, weights.class0 = y_true, curve = TRUE)
cat("PR AUC (Lasso, Validation Set):", pr_1d_test$auc.integral, "\n")
# PR Curve for Elastic Net
pr_3d_test <- pr.curve(scores.class0 = predictions_3d_test, weights.class0 = y_true, curve = TRUE)
cat("PR AUC (Elastic Net, Validation Set):", pr_3d_test$auc.integral, "\n")
# Create Data Frames for Precision-Recall Curves
df_1d <- data.frame(Recall = pr_1d_test$curve[, 1], Precision = pr_1d_test$curve[, 2], Threshold = pr_1d_test$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d_test$curve[, 1], Precision = pr_3d_test$curve[, 2], Threshold = pr_3d_test$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)
ggplot(pr_data, aes(x = Recall, y = Precision, color = Model)) +
geom_line(size = 1) +
labs(
title = "Precision-Recall Curves for Lasso and Elastic Net (Test Set)",
x = "Recall",
y = "Precision",
color = "Model"
) +
theme_minimal()
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning the parameters)
predicted_classes_lasso <- ifelse(predictions_1d_test > 0.1520342, 1, 0)
predicted_classes_EN <- ifelse(predictions_3d_test > 0.1760668, 1, 0)
# Cross validation errors when we were training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)
cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
calculate_balanced_accuracy <- function(conf_matrix) {
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]
TP <- conf_matrix[2, 2]
sensitivity <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)  # Specificity
(sensitivity + specificity) / 2
}
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
balanced_accuracy_EN <- calculate_balanced_accuracy(conf_matrix_EN)
cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
# Define and train the Random Forest model
rf_model <- h2o.randomForest(
x = colnames(training_data)[-which(names(training_data) == "label")],  # Predictors
y = "label",                    # Target variable
training_frame = train_h2o,     # Training data
ntrees = 100,                   # Number of trees
max_depth = 15,                 # Maximum tree depth
mtries = 1000,                   # Features considered per split
min_rows = 20,                  # Minimum rows per leaf
sample_rate = 0.8,              # Row sampling rate
)
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
# Define and train the Random Forest model
rf_model <- h2o.randomForest(
x = features_train,  # Predictors
y = y_train,                    # Target variable
training_frame = train_h2o,     # Training data
ntrees = 100,                   # Number of trees
max_depth = 15,                 # Maximum tree depth
mtries = 1000,                   # Features considered per split
min_rows = 20,                  # Minimum rows per leaf
sample_rate = 0.8,              # Row sampling rate
)
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
# Define and train the Random Forest model
rf_model <- h2o.randomForest(
x = colnames(features_train),  # Predictors
y = "label",                    # Target variable
training_frame = train_h2o,     # Training data
ntrees = 100,                   # Number of trees
max_depth = 15,                 # Maximum tree depth
mtries = 1000,                   # Features considered per split
min_rows = 20,                  # Minimum rows per leaf
sample_rate = 0.8,              # Row sampling rate
)
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
# Define and train the Random Forest model
rf_model <- h2o.randomForest(
x = colnames(features_train),  # Predictors
y = colnames(y_train),                    # Target variable
training_frame = train_h2o,     # Training data
ntrees = 100,                   # Number of trees
max_depth = 15,                 # Maximum tree depth
mtries = 1000,                   # Features considered per split
min_rows = 20,                  # Minimum rows per leaf
sample_rate = 0.8,              # Row sampling rate
)
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
# Define and train the Random Forest model
rf_model <- h2o.randomForest(
x = colnames(features_train),  # Predictors
y = training_data$label,                    # Target variable
training_frame = train_h2o,     # Training data
ntrees = 100,                   # Number of trees
max_depth = 15,                 # Maximum tree depth
mtries = 1000,                   # Features considered per split
min_rows = 20,                  # Minimum rows per leaf
sample_rate = 0.8,              # Row sampling rate
)
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
data <- read.csv("data2.csv.gz")
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
if (data[i, 1] == -1) {
count <- count + 1
}
}
binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")
head(data)
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
if (data[i, 1] == -1) {
data[i, 1] <- 0
} else {
data[i, 1] <- 1
}
}
print(data)
# Calculate row indices for each split
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 15% for validation
# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
# Print dimensions of each set
print(dim(training_data))
print(dim(validation_data))
print(dim(testing_data))
set.seed(123)
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_val <- validation_data[ ,1]
y_true <- testing_data[,1]
#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_val <- validation_data[,-1]
sparse_data_val <- as(features_val, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
set.seed(123)
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
library(PRROC)
# Predict on the Validation Set
# Lasso Model Predictions
predictions_1d_val <- predict(cv_fit_1d, sparse_data_val, s = "lambda.min", type = "response")
# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_val <- predict(cv_fit_3d, sparse_data_val, s = "lambda.min", type = "response")
# Calculate PR Curves Using Validation Labels (y_val)
# PR Curve for Lasso
pr_1d_val <- pr.curve(scores.class0 = predictions_1d_val, weights.class0 = y_val, curve = TRUE)
cat("PR AUC (Lasso, Validation Set):", pr_1d_val$auc.integral, "\n")
# PR Curve for Elastic Net
pr_3d_val <- pr.curve(scores.class0 = predictions_3d_val, weights.class0 = y_val, curve = TRUE)
cat("PR AUC (Elastic Net, Validation Set):", pr_3d_val$auc.integral, "\n")
# Create Data Frames for Precision-Recall Curves
df_1d <- data.frame(Recall = pr_1d_val$curve[, 1], Precision = pr_1d_val$curve[, 2], Threshold = pr_1d_val$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d_val$curve[, 1], Precision = pr_3d_val$curve[, 2], Threshold = pr_3d_val$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)
# Compute F1 Scores and Find Optimal Thresholds
pr_data <- pr_data %>%
mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Avoid division by zero
# Find the Optimal Threshold for Each Model
optimal_thresholds <- pr_data %>%
group_by(Model) %>%
filter(F1 == max(F1)) %>%
summarize(Optimal_Threshold = Threshold[1], Max_F1 = F1[1])
# Print Optimal Thresholds
print(optimal_thresholds)
library(PRROC)
# Predict on the test to see performance
# Lasso Model Predictions
predictions_1d_test <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response")
# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_test <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")
# Calculate PR Curves Using true Labels
# PR Curve for Lasso
pr_1d_test <- pr.curve(scores.class0 = predictions_1d_test, weights.class0 = y_true, curve = TRUE)
cat("PR AUC (Lasso, Validation Set):", pr_1d_test$auc.integral, "\n")
# PR Curve for Elastic Net
pr_3d_test <- pr.curve(scores.class0 = predictions_3d_test, weights.class0 = y_true, curve = TRUE)
cat("PR AUC (Elastic Net, Validation Set):", pr_3d_test$auc.integral, "\n")
# Create Data Frames for Precision-Recall Curves
df_1d <- data.frame(Recall = pr_1d_test$curve[, 1], Precision = pr_1d_test$curve[, 2], Threshold = pr_1d_test$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d_test$curve[, 1], Precision = pr_3d_test$curve[, 2], Threshold = pr_3d_test$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)
ggplot(pr_data, aes(x = Recall, y = Precision, color = Model)) +
geom_line(size = 1) +
labs(
title = "Precision-Recall Curves for Lasso and Elastic Net (Test Set)",
x = "Recall",
y = "Precision",
color = "Model"
) +
theme_minimal()
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning the parameters)
predicted_classes_lasso <- ifelse(predictions_1d_test > 0.1520342, 1, 0)
predicted_classes_EN <- ifelse(predictions_3d_test > 0.1760668, 1, 0)
# Cross validation errors when we were training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)
cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
calculate_balanced_accuracy <- function(conf_matrix) {
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]
TP <- conf_matrix[2, 2]
sensitivity <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)  # Specificity
(sensitivity + specificity) / 2
}
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)
# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)
balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
balanced_accuracy_EN <- calculate_balanced_accuracy(conf_matrix_EN)
cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data[,-1])
