---
title: "Task 1 - ST443 Project"
author: ""
date: "`r Sys.Date()`"
output: html_document
---

```{r libraries, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
seed <- 443
set.seed(seed)
library(ggplot2)
library(tidyverse)
library(MASS)
library(class)
library(pROC)
library(caret) 
library(mboost)   
library(gbm)
library(xgboost)
library(ranger)
library(e1071)
library(lattice)
library(PRROC)
library(glmnet)
```

```{r reading in data, echo=TRUE}
task1data <- read.csv("data1.csv.gz")
task1data$label <- if_else(task1data$label == "TREG", 1, 0)
```

```{r eval_metrics function, echo=TRUE}

eval_metrics <- function(str, conf){
  
  ### function takes as input a string (str) that describes the method, and a 
  ### a confusion matrix to evaluate Accuracy, Balanced, Precision, Recall, 
  ### and F1 score from. 
  ###
  ### Returns a row of a dataframe with the method and the key metrics.
  
  # setup
  TN <- conf[[1]]
  FN <- conf[[2]]
  FP <- conf[[3]]
  TP <- conf[[4]]
  
  # metrics
  accuracy <- 1 - ((FN + FP) / sum(conf))
  BA <- .5 * (TP / (TP + FN)) + .5 * (TN / (TN + FP))
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall + 1e-6)
  
  # output
  return(data.frame("Method" = str,
                    "Accuracy" = accuracy |> round(digits = 3),
                    "Balanced Accuracy" = BA |> round(digits = 3),
                    "F1" = F1 |> round(digits = 3)))
}

```


## T1.1

```{r missing data, echo=TRUE}

nas <- task1data |> is.na() |> colSums() |> table()
if (nas[1] == ncol(task1data)){
  print("We have that there are no missing data in any columns which will be helpful in our analysis")
}
```

```{r overall sparsity, echo=FALSE}
total_zeros <- sum(rowSums(task1data == 0))
total_entries <- nrow(task1data) * ncol(task1data)
(total_zeros / total_entries) |> round(digits = 3)
```

Our given data is reasonably sparse. In fact, below we have shown that approximately 66.2% of this dataset are zero entries. We can explore sparsity across the covariates as well.

```{r sparsity across features, echo=TRUE}
covariate_sparsities <- data.frame(Gene = colnames(task1data)[-1], 
                                   Sparsity = rep(0, ncol(task1data)-1)) 

for (i in 2:ncol(task1data)){
  count <- length(which(task1data[,i] == 0))
  covariate_sparsities$Sparsity[i-1] <- count / nrow(task1data)
}

summary(covariate_sparsities)
ggplot(covariate_sparsities, aes(Sparsity)) + 
  geom_histogram(color = "black", fill = "white" ,bins = 40) + 
  ggtitle("Distribution of Sparsity Rates Across Covariates") + 
  geom_vline(xintercept = mean(covariate_sparsities$Sparsity), color = "red", linetype = "dashed") +
  theme_bw()

barplot(colMeans(task1data[2:ncol(task1data)]))

```


```{r heatmap}

task1matrix <- sapply(task1data, as.numeric) |> as.matrix()

pal <- colorRampPalette(c("red", "yellow"), space = "rgb") 
levelplot(task1matrix, 
          main="Task 1 Data Heatmap",
          xlab=" ", 
          ylab=" ", 
          col.regions=pal(40), 
          cuts=3, 
          at=seq(0,1,0.5))


```

## T1.2

Below we will shuffle the rows in our dataset, then split them into *training*, *validation*, and *test* sets. In the base and PCA model fits, we use *train* and *test*, and for the tuning portion (T1.3), we use the *validation* set.

```{r data setup, echo=TRUE}
task1data <- task1data[sample(1:nrow(task1data)), ]

training_index <- floor(nrow(task1data) * 0.7)  # 70% for training
validation_index <- floor(nrow(task1data) * 0.85)  # Next 15% for validation, leaving 15% for test set

# Split the data into training, validation, and testing sets
task1_train <- task1data[1:training_index, ]  # 70% of the data
task1_validation <- task1data[(training_index + 1):validation_index, ]  # 15% of the data
task1_test <- task1data[(validation_index + 1):nrow(task1data), ]  # Remaining 15% of the data

```

### Base Models

#### LDA

```{r lda fit}
lda_fit <- lda(label ~ ., data = task1_train)
```

```{r lda predict}
lda_pred_full <-  predict(lda_fit, task1_test)
lda_pred <- predict(lda_fit, task1_test)$class
lda_conf_matrix <- table(lda_pred, task1_test$label)
```

#### Logistic Regression

```{r logistic regression, echo=TRUE}
logistic_fit <- glm(label ~ ., data = task1_train, family = binomial)
logistic_probs <-  predict(logistic_fit, newdata = task1_test, type = "response")
```

```{r logit eval}
logistic_pred <-  rep(0, nrow(task1_test))
logistic_pred[logistic_probs > .5] <-  1

logistic_conf_matrix <- table(logistic_pred, task1_test$label)
eval_metrics(str = "Logisitc Regression", conf = logistic_conf_matrix)

```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are more columns/features/covariates than there are instances of either class (TREG & CD4+T). In order for qda() to run properly, we can only have, at a maximum, equal numbers of covariates and counts of either class. At this juncture, we can consider dimension reduction methods, as seen in T1.3. 

```{r qda, echo=TRUE}
#qda_fit <- MASS::qda(label ~ ., data = task1_train, subset = train)
```

#### KNN
```{r knn, echo=TRUE}
knn <- knn3(label ~., data = task1_train, k=1) # base case of k=1, alternatives explored later
knn_preds <- predict(knn, newdata = task1_test, type = "prob")
knn_conf_matrix <- table(knn_preds[,2], task1_test$label)
eval_metrics("KNN", knn_conf_matrix)
```

#### GBDT

```{r GBDT}

gbdt <- gbm(label ~ ., data = task1_train, 
                   distribution = "bernoulli", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001)

gbdt_probs <- predict(gbdt, newdata = task1_test, n.trees = 1000, type = "response")

gbdt_preds <-  rep(0, nrow(task1_test))
gbdt_preds <- ifelse(gbdt_probs > .5, 1, 0)

gbdt_conf_matrix <- table(gbdt_preds, task1_test$label)

```

#### Random Forest

```{r echo=TRUE}

rf <- ranger(label~., 
             data = task1_train, 
             mtry = ncol(task1_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

rf_preds <- predict(rf, data=task1_test)$predictions

rf_conf_matrix <- table(rf_preds, task1_test$label)

```

#### SVM

```{r}
svmfit = svm(label ~ ., data = task1_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = 1)

svm_predictions <- predict(svmfit, newdata = task1_test[2:ncol(task1_test)], probabilities = TRUE)
svm_conf_matrix <- table(svm_predictions, task1_test$label)
```



## T1.2.PCA

```{r pca fit & data}
pca <- prcomp(~., data = task1data[2:ncol(task1data)]) # conducting pca
top10_weights <- pca$rotation[,1:10] 
reduced_data <- pca$x[,1:10] |> as.data.frame() # only keeping top ten components
reduced_data$label <- task1data$label # reassigning labels; PCA maintains row order
```

```{r pca plot, echo=TRUE}
table_pca <- rbind(pca$rotation[,1:20], summary(pca)$importance[,1:20])

par(mfrow=c(1,1))
plot(table_pca['Proportion of Variance',], 
     type = 'l', 
     lwd = 5, 
     col = 'blue', 
     xlim = c(1,20), 
     ylim = c(0,.05),
     main = 'Proportion of Variance Explained by Principal Components', 
     xlab = 'Principal Components', 
     ylab = 'Proportion of Variance Unexplained', 
     axes = TRUE)

```

Same data set splitting as earlier; PCA maintains row order by matrix multiplication.

```{r data setup, echo=TRUE}
reduced_train <- reduced_data[1:training_index,] 
reduced_validation <- reduced_data[(training_index + 1):validation_index,]
reduced_test <- reduced_data[(validation_index + 1):nrow(task1data),] 
```

### Models with PCA

#### LDA

```{r pca lda fit}
pca_lda_fit <- lda(label ~ ., data = reduced_train)
```

```{r pca lda predict}
pca_lda_pred_full <-  predict(pca_lda_fit, reduced_test)
pca_lda_pred <- predict(pca_lda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_lda_conf_matrix <- table(pca_lda_pred, pca_test_labels)
```

```{r pca lda eval, echo=TRUE}
pca_lda_conf_matrix
eval_metrics(str = "LDA", conf = pca_lda_conf_matrix)

```

#### Logistic Regression

```{r pca logistic regression, echo=TRUE}
pca_logistic_fit <- glm(label ~ ., data = reduced_train, family = binomial)
pca_logistic_probs <-  predict(pca_logistic_fit, newdata = reduced_test, type = "response")
```

```{r pca logit eval}
pca_logistic_pred <-  rep(0, nrow(reduced_test))
pca_logistic_pred[pca_logistic_probs > .5] <-  1

# tail(cbind(task1_train$label,logistic_pred))
pca_logistic_conf_matrix <- table(pca_logistic_pred, reduced_test$label)
eval_metrics(str = "Logisitc Regression", conf = pca_logistic_conf_matrix)
```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods.

```{r qda, echo=TRUE}
pca_qda_fit <- MASS::qda(label ~ ., data = reduced_train)

pca_qda_pred_full <-  predict(pca_qda_fit, reduced_test)
pca_qda_pred <- predict(pca_qda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_qda_conf_matrix <- table(pca_qda_pred, pca_test_labels)

```

#### KNN

```{r pca knn, echo=TRUE}

pca_knn <- knn3(label ~., data = reduced_train, k = 1)
pca_knn_preds <- predict(pca_knn, newdata = reduced_test, type = "prob")
pca_knn_conf_matrix <- table(pca_knn_preds[,2], task1_test$label)
```

#### GBDT

```{r pca GBDT}

pca_gbdt <- gbm(label ~ ., data = reduced_train, 
                   distribution = "bernoulli", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001, 
                   cv.folds = 5)

pca_gbdt_probs <- predict(pca_gbdt, newdata = reduced_test, n.trees = 1000, type = "response")

pca_gbdt_preds <-  rep(0, nrow(task1_test))
pca_gbdt_preds <- ifelse(pca_gbdt_probs > .5, 1, 0)

pca_gbdt_conf_matrix <- table(pca_gbdt_preds, reduced_test$label)
```

#### Random Forest

```{r pca rf, echo=TRUE}

pca_rf <- ranger(label~., 
             data = reduced_train, 
             mtry = ncol(reduced_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

pca_rf_preds <- predict(pca_rf, data=reduced_test)$predictions

pca_rf_conf_matrix <- table(pca_rf_preds, reduced_test$label)

eval_metrics("RF", pca_rf_conf_matrix)

```

#### SVM

```{r pca svm}
pca_svmfit = svm(label ~ ., data = reduced_train, 
             kernel = "linear", 
             type = "C-classification",
             cost = 1)

pca_svm_predictions <- predict(pca_svmfit, newdata = reduced_test, probabilities = TRUE)
pca_svm_conf_matrix <- table(pca_svm_predictions, reduced_test$label)
eval_metrics("PCA SVM", pca_svm_conf_matrix)

```


## T1.3

Classifiers to tweak:
- Logistic Regression via Lasso Regularization
- KNN via Tuning *k*
- GBDT via Tuning Depth, Shrinkage, and Decision Threshold

In this section, we bring in the validation set (15% of the data) to tune hyperparameters, settle on an optimal setup, and then reevaluate on the test set. 


#### Logistic Regression

```{r lasso glmnet}

train_matrix <- model.matrix(label~., data = task1_train)[,-1]
test_matrix <- model.matrix(label~., data = task1_test)[,-1]
response <- task1_train$label

lasso <- glmnet(train_matrix, response, family = "binomial", alpha=1)
cv_lasso <- cv.glmnet(train_matrix, response, family = "binomial", type.measure = "class", alpha=1)
optimal_lambda <- cv_lasso$lambda.1se

```


```{r lasso threshold selection and implementation}
validation_matrix <- model.matrix(label~., data = task1_validation)[,-1]

# Predict on the Validation Set
lasso_validation_preds <- predict(lasso, newx = validation_matrix, type = "response", s = optimal_lambda) # from cv_lasso / cv.glmnet() output above

# Calculate PR Curves Using Validation Labels (y_val)
lasso_validation_pr_curve <- pr.curve(scores.class0 = lasso_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)


cat("PR AUC (lasso, Validation Set):", lasso_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
lasso_curve_df <- data.frame(Model = "Logistic lasso Regression", 
                          Recall = lasso_validation_pr_curve$curve[, 1], 
                          Precision = lasso_validation_pr_curve$curve[, 2], 
                          Threshold = lasso_validation_pr_curve$curve[, 3])

lasso_validation_pr_curve |> plot()

# Compute F1 Scores and Find Optimal Thresholds
lasso_curve_df <- lasso_curve_df |> mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # '+ 1e-6' to avoid division by zero

lasso_threshold <- lasso_curve_df[which.max(lasso_curve_df$F1),4]

lasso_probs <- predict(lasso, newx = test_matrix, type = "response", s = optimal_lambda) # from cv_lasso / cv.glmnet() output above

lasso_preds <-  rep(0, length(lasso_probs))
lasso_preds <- ifelse(lasso_probs > lasso_threshold, 1, 0)

lasso_conf_matrix <- table(lasso_preds, task1_test$label)

```


#### KNN

In the *k*-Nearest Neighbor classifier, here we are tweaking the main parameter *k*, the number of neighbors taken into account in fitting the model.

```{r knn k-tuning, echo=TRUE}
knn_validation_preds <- predict(knn, newdata = task1_validation, type = "prob")
knn_validation_conf_matrix <- table(knn_validation_preds[,2], task1_test$label)
knn_metrics <- eval_metrics(str = "1 NN", conf = knn_validation_conf_matrix)

pca_knn_validation_preds <- predict(pca_knn, newdata = reduced_validation[1:ncol(reduced_validation)-1], type = "prob")
pca_knn_validation_conf_matrix <- table(pca_knn_validation_preds[,2], reduced_test$label)
pca_knn_metrics <- eval_metrics(str = "1 NN", conf = pca_knn_validation_conf_matrix)

for (k in 2:20){
  model <- knn3Train(task1_train[2:ncol(task1_train)], 
                     task1_validation[2:ncol(task1_validation)], 
                     task1_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)
  
  pca_model <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                     reduced_validation[2:ncol(reduced_validation)], 
                     reduced_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)  

  conf_matrix <- table(model, task1_validation$label)
  pca_conf_matrix <- table(pca_model, reduced_validation$label)
  
  
  knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), conf_matrix)
  pca_knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), pca_conf_matrix)
  
  print(k)
}

ggplot(knn_metrics, aes(x=(1:nrow(knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(knn_metrics$F1), linetype = "dashed", color = "red") + 
  xlim(1,nrow(knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

ggplot(pca_knn_metrics, aes(x=(1:nrow(pca_knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(pca_knn_metrics$F1), linetype = "dashed", color = "red") + 
  geom_hline(yintercept = max(knn_metrics$F1), linetype = "solid", color = "dark grey") + 
  annotate("text",x = 17.5 , y = max(knn_metrics$F1) -.015, label = "Optimal kNN on Full Data", color = "dark grey") +
  xlim(1,nrow(pca_knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  # ggtitle("Selecting k to Maximize F1 Score for Dimension-Reduced Models") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

```

```{r final knn, echo=TRUE}
# KNN Model Predictions

tuned_1nn <- knn3Train(task1_train[2:ncol(task1_train)], 
                       task1_test[2:ncol(task1_test)], 
                       task1_train$label, 
                       k=6, 
                       prob = TRUE, 
                       use.all=TRUE)

tuned_1nn_conf_matrix <- table(tuned_1nn, task1_test$label)

pca_tuned_knn <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                       reduced_test[2:ncol(reduced_test)], 
                       reduced_train$label, 
                       k=5, 
                       prob = TRUE, 
                       use.all=TRUE)

pca_tuned_knn_conf_matrix <- table(pca_tuned_knn, reduced_test$label)

```


#### GBDT

One classifier to improve upon is Gradient Boosted Decision Trees. First, we will tune the shrinkage and interaction (tree) depth hyperparameters, and then we will use the optimal setup we found to tune the right decision threshold. 

```{r, GBTD hyperparameter tuning, echo=TRUE}
gbdt_metrics <- eval_metrics("Base Model", gbdt_conf_matrix)
# Define grid of lambda (shrinkage) values to evaluate
lambdas <- c(.001, .01, .025, .05)
depths <- c(2,3,4,5,6)
# Loop over each lambda value
for (lambda in lambdas) {
  for (depth in depths){
    # Train the gbm model with the current lambda (shrinkage) value
    gbm_model <- gbm(label ~ ., data = task1_train, 
                     distribution = "bernoulli", 
                     n.trees = 1000, 
                     interaction.depth = depth, 
                     shrinkage = lambda)
    
    probabilities <- predict(gbm_model, newdata = task1_test, n.trees = 1000, type = "response")
    
    predictions <- rep(0, nrow(task1_test))
    predictions <- ifelse(probabilities > .5, 1, 0)
  
    conf_matrix <- table(predictions, task1_test$label)
    
    gbdt_metrics <- rbind(gbdt_metrics, eval_metrics(paste("lambda:", as.character(lambda), ";", 
                                                           "depth:", as.character(depth)), conf_matrix))
    print(depth)
    print(lambda)
  }
}

gbdt_metrics$index <- 1:nrow(gbdt_metrics) # indexes for different combinations of hyperparameters

ggplot(gbdt_metrics[2:nrow(gbdt_metrics),], aes(x=index, F1)) + 
  geom_line() +
  geom_point() +
  geom_hline(yintercept = gbdt_metrics$F1[1], linetype = "solid", color = "dark grey") + 
  annotate("text", x=1.7, y=gbdt_metrics$F1[1]+.005, label = "Base Model", color = "dark grey") +
  geom_vline(xintercept = which.max(gbdt_metrics$F1), linetype = "dashed", color = "red") + 
  xlim(1,nrow(gbdt_metrics)) + 
  xlab("Setup Index") + 
  ylab("F1 Score") +
  # ggtitle("Selecting Shrinkage and Depth to Maximize F1 Score") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 


```


```{r gbdt parameters}
paste("Optimal setup for our Gradient Boosted Decision Tress is", gbdt_metrics[which.max(gbdt_metrics$F1),1])
```


```{r gbdt threshold selection}

# Fit model with ideal hyperparameters on training data

tuned_gbdt <- gbm(label ~ ., data = task1_train, 
                     distribution = "bernoulli", 
                     n.trees = 1000, 
                     interaction.depth = 3, 
                     shrinkage = .05)

# Predict on the Validation Set
gbdt_validation_preds <- predict(tuned_gbdt, newdata = task1_validation[2:ncol(task1_validation)], n.trees = 1000, type = "response") # $predictions

# Calculate PR Curves Using Validation Labels (y_val)
gbdt_validation_pr_curve <- pr.curve(scores.class0 = gbdt_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)


cat("PR AUC (GBDT, Validation Set):", gbdt_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
gbdt_curve_df <- data.frame(Model = "Gradient Boosted Decision Trees", 
                          Recall = gbdt_validation_pr_curve$curve[, 1], 
                          Precision = gbdt_validation_pr_curve$curve[, 2], 
                          Threshold = gbdt_validation_pr_curve$curve[, 3])

gbdt_validation_pr_curve |> plot()

```


```{r gbdt optimal threshold implementation}

# Compute F1 Scores and Find Optimal Thresholds
gbdt_curve_df <- gbdt_curve_df |> mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # '+ 1e-6' to avoid division by zero

gbdt_threshold <- gbdt_curve_df[which.max(gbdt_curve_df$F1),4]

tuned_gbdt_probs <- predict(tuned_gbdt, newdata = task1_test, n.trees = 1000, type = "response")

tuned_gbdt_preds <-  rep(0, nrow(task1_test))
tuned_gbdt_preds <- ifelse(tuned_gbdt_probs > gbdt_threshold, 1, 0)

tuned_gbdt_conf_matrix <- table(tuned_gbdt_preds, task1_test$label)

```


## Model Evaluation

```{r making summary tables}

# need to add ROC / AUC info

base_summary_table <- rbind(eval_metrics("LDA", lda_conf_matrix), 
                       eval_metrics("Logistic Regression", logistic_conf_matrix),
                       c("QDA", rep(NA,3)),
                       eval_metrics("1NN", knn_conf_matrix),
                       eval_metrics("GBDT", gbdt_conf_matrix),
                       eval_metrics("Random Forest", rf_conf_matrix),
                       eval_metrics("SVM", svm_conf_matrix))

pca_summary_table <- rbind(eval_metrics("LDA", pca_lda_conf_matrix), 
                       eval_metrics("Logistic Regression", pca_logistic_conf_matrix),
                       eval_metrics("QDA", pca_qda_conf_matrix),
                       eval_metrics("KNN", pca_knn_conf_matrix),
                       eval_metrics("GBDT", pca_gbdt_conf_matrix),
                       eval_metrics("Random Forest", pca_rf_conf_matrix),
                       eval_metrics("SVM", pca_svm_conf_matrix))


tuned_summary_table <- rbind(eval_metrics("9NN", pca_tuned_knn_conf_matrix),
                       eval_metrics("GBDT with Optimal Threshold", tuned_gbdt_conf_matrix), 
                       eval_metrics("Logistic with Lasso ", lasso_conf_matrix))


```


```{r base auc curves, echo=TRUE}


lda_roc_curve <- roc(task1_test$label, lda_pred |> as.numeric(), levels = c(0,1), direction = "<")
lda_auc <- auc(lda_roc_curve)

# qda_roc_curve <- roc(task1_test$label, qda_pred |> as.numeric(), levels = c(0,1), direction = "<")
# auc(qda_roc_curve)

logistic_roc_curve <- roc(task1_test$label, logistic_probs |> as.numeric(), levels = c(0,1), direction = "<")
logisitc_auc <- auc(logistic_roc_curve)

knn_roc_curve <- roc(task1_test$label, knn_preds[,2] |> as.numeric(), levels = c(0,1), direction = "<")
knn_auc <- auc(knn_roc_curve)

gbdt_roc_curve <- roc(task1_test$label, gbdt_probs |> as.numeric(), levels = c(0,1), direction = "<")
gbdt_auc <- auc(gbdt_roc_curve)

rf_roc_curve <- roc(task1_test$label, rf_preds |> as.numeric(), levels = c(0,1), direction = "<")
rf_auc <- auc(rf_roc_curve)

svm_roc_curve <- roc(task1_test$label, svm_predictions |> as.numeric(), levels = c(0,1), direction = "<")
svm_auc <- auc(svm_roc_curve)

base_summary_table$AUC <- c(lda_auc, logisitc_auc, NA, knn_auc, gbdt_auc, rf_auc, svm_auc) |> round(digits = 3)
```

```{r pca auc curves}
pca_lda_roc_curve <- roc(reduced_test$label, pca_lda_pred |> as.numeric(), levels = c(0,1), direction = "<")
pca_lda_auc <- auc(pca_lda_roc_curve)

pca_logistic_roc_curve <- roc(task1_test$label, pca_logistic_probs |> as.numeric(), levels = c(0,1), direction = "<")
pca_logisitc_auc <- auc(pca_logistic_roc_curve)

pca_qda_roc_curve <- roc(task1_test$label, pca_qda_pred |> as.numeric(), levels = c(0,1), direction = "<")
pca_qda_auc <- auc(pca_qda_roc_curve)

pca_knn_roc_curve <- roc(task1_test$label, pca_knn_preds[,2] |> as.numeric(), levels = c(0,1), direction = "<")
pca_knn_auc <- auc(pca_knn_roc_curve)

pca_gbdt_roc_curve <- roc(task1_test$label, pca_gbdt_probs |> as.numeric(), levels = c(0,1), direction = "<")
pca_gbdt_auc <- auc(pca_gbdt_roc_curve)

pca_rf_roc_curve <- roc(task1_test$label, pca_rf_preds |> as.numeric(), levels = c(0,1), direction = "<")
pca_rf_auc <- auc(pca_rf_roc_curve)

pca_svm_roc_curve <- roc(task1_test$label, pca_svm_predictions |> as.numeric(), levels = c(0,1), direction = "<")
pca_svm_auc <- auc(pca_svm_roc_curve)


pca_summary_table$AUC <- c(pca_lda_auc, pca_logisitc_auc, pca_qda_auc, pca_knn_auc, pca_gbdt_auc, pca_rf_auc, pca_svm_auc) |> round(digits = 3)
```

```{r tuned auc curves}
pca_knn_roc_curve <- roc(reduced_test$label, pca_tuned_knn |> as.numeric(), levels = c(0,1), direction = "<")
pca_tuned_knn_auc <- auc(pca_knn_roc_curve)

tuned_gbdt_roc_curve <- roc(reduced_test$label, tuned_gbdt_preds |> as.numeric(), levels = c(0,1), direction = "<")
tuned_gbdt_auc <- auc(tuned_gbdt_roc_curve)

lasso_roc_curve <- roc(task1_test$label, lasso_preds |> as.numeric(), levels = c(0,1), direction = "<")
lasso_auc <- auc(lasso_roc_curve)

tuned_summary_table$AUC <- c(pca_tuned_knn_auc, tuned_gbdt_auc, lasso_auc) |> round(digits = 3)

```


```{r final tables}
base_summary_table

pca_summary_table

tuned_summary_table
```

## T1.4

### mypredict()

```{r mypredict() function}

mypredict <- function(){
  
  ### function takes nothing as input
  ###
  ### returns predictions, one per row, in the same order as test.csv.gz
  
  # shuffling data 
  data <- read.csv("test.csv.gz") # test.csv.gz
  data$label <- if_else(data$label == "TREG", 1, 0)
  
  # predict on top model from T1.3
  probabilities <- predict(tuned_gbdt, newdata = data[,-1], n.trees = 1000, type = "response")
  predictions <- rep(0, nrow(data))
  predictions <- ifelse(probabilities > gbdt_threshold, 1, 0) # threshold defined as 0.4698173
  
  write.table(predictions |> as.data.frame(), 
                         file = "predictions.txt", 
                         quote = FALSE, 
                         row.names = FALSE, 
                         col.names = FALSE)
  
}


```







